<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>1. Setup</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p>This data tutorial provides instruction on working with two different NEON 
data products to estimate tree height: </p>

<ul>
<li><strong>DP3.30015.001, Ecosystem structure</strong>, aka Canopy Height Model (CHM) </li>
<li><strong>DP1.10098.001, Woody plant vegetation structure</strong></li>
</ul>

<p>The <a href="https://data.neonscience.org/data-products/DP3.30015.001" target="_blank">CHM data</a> are derived from the Lidar point cloud data collected by the remote sensing platform. 
The <a href="https://data.neonscience.org/data-products/DP1.10098.001" target="_blank">vegetation structure data</a> are collected by by field staff on the ground. We will be using data from the Wind River Experimental Forest NEON field site located in Washington state. The 
predominant vegetation there are tall evergreen conifers. </p>

<p>If you are coming to this exercise after following tutorials on data 
download and formatting, and therefore already have the needed data, 
skip ahead to section 4.</p>

<div id="ds-objectives" markdown="1">

## Things Youâ€™ll Need To Complete This Tutorial
You will need the most current version of R loaded on your computer to complete this tutorial.

</div>

<h2>1. Setup</h2>

<p>Start by installing and loading packages (if necessary) and setting options. One of the 
packages we&#39;ll be using, <code>geoNEON</code>, is only available via GitHub, so it&#39;s 
installed using the <code>devtools</code> package. The other packages can be 
installed directly from CRAN.</p>

<pre><code>options(stringsAsFactors=F)

# install.packages(&quot;neonUtilities&quot;)
# install.packages(&quot;sp&quot;)
# install.packages(&quot;raster&quot;)
# install.packages(&quot;devtools&quot;)
# devtools::install_github(&quot;NEONScience/NEON-geolocation/geoNEON&quot;)

library(sp)
library(raster)
library(neonUtilities)
library(geoNEON)
</code></pre>

<h2>2. Vegetation structure data</h2>

<p>Download the vegetation structure data using the <code>loadByProduct()</code> function in
the <code>neonUtilities</code> package. Inputs needed to the function are:</p>

<ul>
<li><code>dpID</code>: data product ID; woody vegetation structure = DP1.10098.001</li>
<li><code>site</code>: 4-letter site code; Wind River = WREF</li>
<li><code>package</code>: basic or expanded; we&#39;ll download basic here</li>
<li><code>check.size</code>: should this function prompt the user with an estimated download size? Set to <code>FALSE</code> here for ease of processing as a script, but good to leave as default <code>True</code> when downloading a dataset for the first time.</li>
</ul>

<p>Refer to the <a href="https://www.neonscience.org/neonDataStackR" target="_blank">tutorial</a> 
for the <code>neonUtilities</code> package for more details if desired.</p>

<pre><code>veglist &lt;- loadByProduct(dpID=&quot;DP1.10098.001&quot;, 
                         site=&quot;WREF&quot;, 
                         package=&quot;basic&quot;, 
                         check.size = FALSE)

## Finding available files
## 
</code></pre>

<p>|<br/>
  |                                                                                  |   0%
  |<br/>
  |==============                                                                    |  17%
  |<br/>
  |===========================                                                       |  33%
  |<br/>
  |=========================================                                         |  50%
  |<br/>
  |=======================================================                           |  67%
  |<br/>
  |====================================================================              |  83%
  |<br/>
  |==================================================================================| 100%
    ## 
    ## Downloading files totaling approximately 0.321278 MB
    ## Downloading 6 files
    ## 
  |<br/>
  |                                                                                  |   0%
  |<br/>
  |================                                                                  |  20%
  |<br/>
  |=================================                                                 |  40%
  |<br/>
  |=================================================                                 |  60%
  |<br/>
  |==================================================================                |  80%
  |<br/>
  |==================================================================================| 100%
    ## 
    ## Unpacking zip files using 1 cores.
    ## Stacking operation across a single core.
    ## Stacking table vst_apparentindividual
    ## Stacking table vst_mappingandtagging
    ## Stacking table vst_perplotperyear
    ## Copied the most recent publication of validation file to /stackedFiles
    ## Copied the most recent publication of categoricalCodes file to /stackedFiles
    ## Copied the most recent publication of variable definition file to /stackedFiles
    ## Finished: Stacked 3 data tables and 3 metadata tables!
    ## Stacking took 0.1722422 secs</p>

<p>Use the <code>getLocTOS()</code> function in the <code>geoNEON</code> package to get 
precise locations for the tagged plants. Refer to the package 
documentation for more details.</p>

<pre><code>vegmap &lt;- getLocTOS(veglist$vst_mappingandtagging, 
                          &quot;vst_mappingandtagging&quot;)

## 
</code></pre>

<p>|<br/>
  |                                                                                  |   0%
  |<br/>
  |                                                                                  |   1%
  |<br/>
  |=                                                                                 |   1%
  |<br/>
  |=                                                                                 |   2%
  |<br/>
  |==                                                                                |   2%
  |<br/>
  |==                                                                                |   3%
  |<br/>
  |===                                                                               |   3%
  |<br/>
  |===                                                                               |   4%
  |<br/>
  |====                                                                              |   4%
  |<br/>
  |====                                                                              |   5%
  |<br/>
  |=====                                                                             |   6%
  |<br/>
  |======                                                                            |   7%
  |<br/>
  |======                                                                            |   8%
  |<br/>
  |=======                                                                           |   8%
  |<br/>
  |=======                                                                           |   9%
  |<br/>
  |========                                                                          |  10%
  |<br/>
  |=========                                                                         |  11%
  |<br/>
  |==========                                                                        |  12%
  |<br/>
  |===========                                                                       |  13%
  |<br/>
  |============                                                                      |  14%
  |<br/>
  |============                                                                      |  15%
  |<br/>
  |=============                                                                     |  16%
  |<br/>
  |==============                                                                    |  17%
  |<br/>
  |===============                                                                   |  18%
  |<br/>
  |===============                                                                   |  19%
  |<br/>
  |================                                                                  |  19%
  |<br/>
  |================                                                                  |  20%
  |<br/>
  |=================                                                                 |  20%
  |<br/>
  |=================                                                                 |  21%
  |<br/>
  |==================                                                                |  21%
  |<br/>
  |==================                                                                |  22%
  |<br/>
  |===================                                                               |  23%
  |<br/>
  |===================                                                               |  24%
  |<br/>
  |====================                                                              |  24%
  |<br/>
  |====================                                                              |  25%
  |<br/>
  |=====================                                                             |  25%
  |<br/>
  |=====================                                                             |  26%
  |<br/>
  |======================                                                            |  26%
  |<br/>
  |======================                                                            |  27%
  |<br/>
  |=======================                                                           |  28%
  |<br/>
  |=======================                                                           |  29%
  |<br/>
  |========================                                                          |  29%
  |<br/>
  |========================                                                          |  30%
  |<br/>
  |=========================                                                         |  30%
  |<br/>
  |=========================                                                         |  31%
  |<br/>
  |==========================                                                        |  31%
  |<br/>
  |==========================                                                        |  32%
  |<br/>
  |===========================                                                       |  33%
  |<br/>
  |============================                                                      |  34%
  |<br/>
  |=============================                                                     |  35%
  |<br/>
  |=============================                                                     |  36%
  |<br/>
  |==============================                                                    |  37%
  |<br/>
  |===============================                                                   |  38%
  |<br/>
  |================================                                                  |  39%
  |<br/>
  |=================================                                                 |  40%
  |<br/>
  |==================================                                                |  41%
  |<br/>
  |==================================                                                |  42%
  |<br/>
  |===================================                                               |  42%
  |<br/>
  |===================================                                               |  43%
  |<br/>
  |====================================                                              |  44%
  |<br/>
  |=====================================                                             |  45%
  |<br/>
  |=====================================                                             |  46%
  |<br/>
  |======================================                                            |  46%
  |<br/>
  |======================================                                            |  47%
  |<br/>
  |=======================================                                           |  47%
  |<br/>
  |=======================================                                           |  48%
  |<br/>
  |========================================                                          |  48%
  |<br/>
  |========================================                                          |  49%
  |<br/>
  |=========================================                                         |  49%
  |<br/>
  |=========================================                                         |  50%
  |<br/>
  |=========================================                                         |  51%
  |<br/>
  |==========================================                                        |  51%
  |<br/>
  |==========================================                                        |  52%
  |<br/>
  |===========================================                                       |  52%
  |<br/>
  |===========================================                                       |  53%
  |<br/>
  |============================================                                      |  53%
  |<br/>
  |============================================                                      |  54%
  |<br/>
  |=============================================                                     |  54%
  |<br/>
  |=============================================                                     |  55%
  |<br/>
  |==============================================                                    |  56%
  |<br/>
  |===============================================                                   |  57%
  |<br/>
  |===============================================                                   |  58%
  |<br/>
  |================================================                                  |  58%
  |<br/>
  |================================================                                  |  59%
  |<br/>
  |=================================================                                 |  60%
  |<br/>
  |==================================================                                |  61%
  |<br/>
  |===================================================                               |  62%
  |<br/>
  |====================================================                              |  63%
  |<br/>
  |=====================================================                             |  64%
  |<br/>
  |=====================================================                             |  65%
  |<br/>
  |======================================================                            |  66%
  |<br/>
  |=======================================================                           |  67%
  |<br/>
  |========================================================                          |  68%
  |<br/>
  |========================================================                          |  69%
  |<br/>
  |=========================================================                         |  69%
  |<br/>
  |=========================================================                         |  70%
  |<br/>
  |==========================================================                        |  70%
  |<br/>
  |==========================================================                        |  71%
  |<br/>
  |===========================================================                       |  71%
  |<br/>
  |===========================================================                       |  72%
  |<br/>
  |============================================================                      |  73%
  |<br/>
  |============================================================                      |  74%
  |<br/>
  |=============================================================                     |  74%
  |<br/>
  |=============================================================                     |  75%
  |<br/>
  |==============================================================                    |  75%
  |<br/>
  |==============================================================                    |  76%
  |<br/>
  |===============================================================                   |  76%
  |<br/>
  |===============================================================                   |  77%
  |<br/>
  |================================================================                  |  78%
  |<br/>
  |================================================================                  |  79%
  |<br/>
  |=================================================================                 |  79%
  |<br/>
  |=================================================================                 |  80%
  |<br/>
  |==================================================================                |  80%
  |<br/>
  |==================================================================                |  81%
  |<br/>
  |===================================================================               |  81%
  |<br/>
  |===================================================================               |  82%
  |<br/>
  |====================================================================              |  83%
  |<br/>
  |=====================================================================             |  84%
  |<br/>
  |======================================================================            |  85%
  |<br/>
  |======================================================================            |  86%
  |<br/>
  |=======================================================================           |  87%
  |<br/>
  |========================================================================          |  88%
  |<br/>
  |=========================================================================         |  89%
  |<br/>
  |==========================================================================        |  90%
  |<br/>
  |===========================================================================       |  91%
  |<br/>
  |===========================================================================       |  92%
  |<br/>
  |============================================================================      |  92%
  |<br/>
  |============================================================================      |  93%
  |<br/>
  |=============================================================================     |  94%
  |<br/>
  |==============================================================================    |  95%
  |<br/>
  |==============================================================================    |  96%
  |<br/>
  |===============================================================================   |  96%
  |<br/>
  |===============================================================================   |  97%
  |<br/>
  |================================================================================  |  97%
  |<br/>
  |================================================================================  |  98%
  |<br/>
  |================================================================================= |  98%
  |<br/>
  |================================================================================= |  99%
  |<br/>
  |==================================================================================|  99%
  |<br/>
  |==================================================================================| 100%</p>

<p>Merge the mapped locations of individuals (the <code>vst_mappingandtagging</code> table) 
with the annual measurements of height, diameter, etc (the 
<code>vst_apparentindividual</code> table). The two tables join on <code>individualID</code>, 
the identifier for each tagged plant, but we&#39;ll include <code>namedLocation</code>, 
<code>domainID</code>, <code>siteID</code>, and <code>plotID</code> in the list of variables to merge on, to 
avoid ending up with duplicates of each of those columns. Refer to the 
variables table and to the <a href="https://data.neonscience.org/api/v0/documents/NEON_vegStructure_userGuide_vA" target="_blank">Data Product User Guide</a> 
for Woody plant vegetation structure for more 
information about the contents of each data table.</p>

<pre><code>veg &lt;- merge(veglist$vst_apparentindividual, vegmap, 
             by=c(&quot;individualID&quot;,&quot;namedLocation&quot;,
                  &quot;domainID&quot;,&quot;siteID&quot;,&quot;plotID&quot;))
</code></pre>

<p>Let&#39;s see what the data look like! Make a stem map of the plants in 
plot WREF_075. Note that the <code>circles</code> argument of the <code>symbols()</code> function expects a radius, but 
stemDiameter is just that, a diameter, so we will need to divide by two. 
stemDiameter is also in centimeters, but the mapping scale is in meters, 
so we need to divide by 100 also to get the scale right.</p>

<pre><code>symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], 
        veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], 
        circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, 
        inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/plot-1-1.png" alt=" "/></p>

<p>And now overlay the estimated uncertainty in the location of each stem, 
in blue:</p>

<pre><code>symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], 
        veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], 
        circles=veg$stemDiameter[which(veg$plotID==&quot;WREF_075&quot;)]/100/2, 
        inches=F, xlab=&quot;Easting&quot;, ylab=&quot;Northing&quot;)
symbols(veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], 
        veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)], 
        circles=veg$adjCoordinateUncertainty[which(veg$plotID==&quot;WREF_075&quot;)], 
        inches=F, add=T, fg=&quot;lightblue&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/plot-2-1.png" alt=" "/></p>

<h2>3. Canopy height model data</h2>

<p>Now we&#39;ll download the CHM tile corresponding to plot WREF_075. Several 
other plots are also covered by this tile. We could download all tiles 
that contain vegetation structure plots, but in this exercise we&#39;re 
sticking to one tile to limit download size and processing time.</p>

<p>The <code>tileByAOP()</code> function in the <code>neonUtilities</code> package allows for 
download of remote sensing tiles based on easting and northing 
coordinates, so we&#39;ll give it the coordinates of plot WREF_075 and 
the data product ID, DP3.30015.001.</p>

<p>The download will include several metadata files as well as the data 
tile. Load the data tile into the environment using the <code>raster</code> package.</p>

<pre><code>byTileAOP(dpID=&quot;DP3.30015.001&quot;, site=&quot;WREF&quot;, year=&quot;2017&quot;, 
          easting=veg$adjEasting[which(veg$plotID==&quot;WREF_075&quot;)], 
          northing=veg$adjNorthing[which(veg$plotID==&quot;WREF_075&quot;)],
          check.size = FALSE, savepath=&quot;/Users/olearyd/Git/data&quot;)

## Downloading files totaling approximately 4.035953 MB 
## Downloading 6 files
## 
</code></pre>

<p>|<br/>
  |                                                                                  |   0%
  |<br/>
  |================                                                                  |  20%
  |<br/>
  |=================================                                                 |  40%
  |<br/>
  |=================================================                                 |  60%
  |<br/>
  |==================================================================                |  80%
  |<br/>
  |==================================================================================| 100%
    ## Successfully downloaded  6  files.
    ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.kml downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/kmls
    ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shx downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps
    ## NEON_D16_WREF_DP3_580000_5075000_CHM.tif downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif
    ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.shp downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps
    ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.dbf downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps
    ## NEON_D16_WREF_DP1_580000_5075000_classified_point_cloud.prj downloaded to /Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/Metadata/DiscreteLidar/TileBoundary/shps</p>

<pre><code>chm &lt;- raster(&quot;/Users/olearyd/Git/data/DP3.30015.001/2017/FullSite/D16/2017_WREF_1/L3/DiscreteLidar/CanopyHeightModelGtif/NEON_D16_WREF_DP3_580000_5075000_CHM.tif&quot;)
</code></pre>

<p>Let&#39;s view the tile.</p>

<pre><code>plot(chm, col=topo.colors(5))
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/plot-chm-1.png" alt=" "/></p>

<h2>4. Comparing the two datasets</h2>

<p>Now we have the heights of individual trees measured from the ground, and 
the height of the top surface of the canopy, measured from the air. There 
are many different ways to make a comparison between these two 
datasets! This section will walk through three different approaches.</p>

<p>First, subset the vegetation structure data to only the individuals that fall 
within this tile, using the <code>extent()</code> function from the raster package.</p>

<p>This step isn&#39;t strictly necessary, but it will make the processing faster.</p>

<pre><code>vegsub &lt;- veg[which(veg$adjEasting &gt;= extent(chm)[1] &amp;
                      veg$adjEasting &lt;= extent(chm)[2] &amp;
                      veg$adjNorthing &gt;= extent(chm)[3] &amp; 
                      veg$adjNorthing &lt;= extent(chm)[4]),]
</code></pre>

<p>Starting with a very simple first pass: use the <code>extract()</code> function 
from the <code>raster</code> package to get the CHM value matching the coordinates 
of each mapped plant. Include a buffer equal to the uncertainty in the 
plant&#39;s location, and extract the highest CHM value within the buffer. 
Then make a scatter plot of each tree&#39;s height vs. the CHM value at its 
location.</p>

<pre><code>bufferCHM &lt;- extract(chm, 
                     cbind(vegsub$adjEasting,
                           vegsub$adjNorthing),
                     buffer=vegsub$adjCoordinateUncertainty, 
                     fun=max)
plot(bufferCHM~vegsub$height, pch=20, xlab=&quot;Height&quot;, 
     ylab=&quot;Canopy height model&quot;)
lines(c(0,50), c(0,50), col=&quot;grey&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/buffer-chm-1.png" alt=" "/></p>

<pre><code>cor(bufferCHM,vegsub$height, use=&quot;complete&quot;)

## [1] 0.3650552
</code></pre>

<p>There are a lot of points clustered on the 1-1 line, but there is also a 
cloud of points above the line, where the measured height is lower than 
the canopy height model at the same coordinates. This makes sense, because 
we made no attempt to filter out the understory. There are likely many 
plants measured in the vegetation structure data that are not at the top 
of the canopy, and the CHM sees only the top surface of the canopy.</p>

<p>How to exclude understory plants from this analysis? Again, there are many 
possible approaches. We&#39;ll try out two, one map-centric and one 
tree-centric.</p>

<p>Starting with the map-centric approach: select a pixel size, and aggregate 
both the vegetation structure data and the CHM data to find the tallest point 
in each pixel. Let&#39;s try this with 10m pixels.</p>

<p>Start by rounding the coordinates of the vegetation structure data, to create 
10m bins. Use <code>floor()</code> instead of <code>round()</code> so each tree ends up in the pixel 
with the same numbering as the raster pixels (the rasters/pixels are 
numbered by their southwest corners).</p>

<pre><code>easting10 &lt;- 10*floor(vegsub$adjEasting/10)
northing10 &lt;- 10*floor(vegsub$adjNorthing/10)
vegsub &lt;- cbind(vegsub, easting10, northing10)
</code></pre>

<p>Use the <code>aggregate()</code> function to get the tallest tree in each 10m bin.</p>

<pre><code>vegbin &lt;- stats::aggregate(vegsub, by=list(vegsub$easting10, vegsub$northing10), FUN=max)
</code></pre>

<p>To get the CHM values for the 10m bins, use the <code>raster</code> package version 
of the <code>aggregate()</code> function. Let&#39;s take a look at the lower-resolution 
image we get as a result.</p>

<pre><code>CHM10 &lt;- raster::aggregate(chm, fact=10, fun=max)
plot(CHM10, col=topo.colors(5))
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/CHM-10-1.png" alt=" "/></p>

<p>Use the <code>extract()</code> function again to get the values from each pixel. We 
don&#39;t need a buffer this time, since we&#39;ve put both datasets onto the same 
grid. But our grids are numbered by the corners, so add 5 to each tree 
coordinate to make sure it&#39;s in the correct pixel.</p>

<pre><code>vegbin$easting10 &lt;- vegbin$easting10+5
vegbin$northing10 &lt;- vegbin$northing10+5
binCHM &lt;- extract(CHM10, cbind(vegbin$easting10, 
                               vegbin$northing10))
plot(binCHM~vegbin$height, pch=20, 
     xlab=&quot;Height&quot;, ylab=&quot;Canopy height model&quot;)
lines(c(0,50), c(0,50), col=&quot;grey&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/adj-tree-coord-1.png" alt=" "/></p>

<pre><code>cor(binCHM, vegbin$height, use=&quot;complete&quot;)

## [1] 0.3565511
</code></pre>

<p>The understory points are thinned out substantially, but so are the rest. 
We&#39;ve lost a lot of data by going to a lower resolution.</p>

<p>Let&#39;s try and see if we can identify the tallest trees by another approach, 
using the trees as the starting point instead of map area. Start by sorting 
the veg structure data by height.</p>

<pre><code>vegsub &lt;- vegsub[order(vegsub$height, decreasing=T),]
</code></pre>

<p>Now, for each tree, let&#39;s estimate which nearby trees might be beneath 
its canopy, and discard those points. To do this:</p>

<ol>
<li>Calculate the distance of each tree from the target tree.</li>
<li>Pick a reasonable estimate for canopy size, and discard shorter trees 
within that radius. The radius I used is 0.3 times the height, based on 
some rudimentary googling about Douglas fir allometry. It could definitely 
be improved on!</li>
<li><p>Iterate over all trees.</p>

<p>vegfil &lt;- vegsub
for(i in 1:nrow(vegsub)) {
    if(is.na(vegfil$height[i]))
        next
    dist &lt;- sqrt((vegsub$adjEasting[i]-vegsub$adjEasting)<sup>2</sup> + 
                (vegsub$adjNorthing[i]-vegsub$adjNorthing)<sup>2)</sup>
    vegfil$height[which(dist&lt;0.3*vegsub$height[i] &amp; 
                        vegsub$height&lt;vegsub$height[i])] &lt;- NA
}</p>

<p>vegfil &lt;- vegfil[which(!is.na(vegfil$height)),]</p></li>
</ol>

<p>Now extract the raster values, as above. Let&#39;s also increase the buffer size 
a bit, to better account for the uncertainty in the Lidar data as well as 
the uncertainty in the ground locations.</p>

<pre><code>filterCHM &lt;- extract(chm, cbind(vegfil$adjEasting, vegfil$adjNorthing),
                         buffer=vegfil$adjCoordinateUncertainty+1, fun=max)
plot(filterCHM~vegfil$height, pch=20, 
     xlab=&quot;Height&quot;, ylab=&quot;Canopy height model&quot;)
lines(c(0,50), c(0,50), col=&quot;grey&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/filter-chm-1.png" alt=" "/></p>

<pre><code>cor(filterCHM,vegfil$height)

## [1] 0.7273229
</code></pre>

<p>This is quite a bit better! There are still several understory points we 
failed to exclude, but we were able to filter out most of the understory 
without losing so many overstory points.</p>

<p>Let&#39;s try one last thing. The <code>plantStatus</code> field in the veg structure data 
indicates whether a plant is dead, broken, or otherwise damaged. In theory, 
a dead or broken tree can still be the tallest thing around, but it&#39;s less 
likely, and it&#39;s also less likely to get a good Lidar return. Exclude all 
trees that aren&#39;t alive:</p>

<pre><code>vegfil &lt;- vegfil[which(vegfil$plantStatus==&quot;Live&quot;),]
filterCHM &lt;- extract(chm, cbind(vegfil$adjEasting, vegfil$adjNorthing),
                         buffer=vegfil$adjCoordinateUncertainty+1, fun=max)
plot(filterCHM~vegfil$height, pch=20, 
     xlab=&quot;Height&quot;, ylab=&quot;Canopy height model&quot;)
lines(c(0,50), c(0,50), col=&quot;grey&quot;)
</code></pre>

<p><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/R/Lidar/lidar-topography/veg_structure_and_chm/rfigs/live-trees-1.png" alt=" "/></p>

<pre><code>cor(filterCHM,vegfil$height)

## [1] 0.8135262
</code></pre>

<p>Nice!</p>

<p>One final note: however we slice the data, there is a noticeable bias 
even in the strongly correlated values. The CHM heights are generally a 
bit shorter than the ground-based estimates of tree height. There are 
two biases in the CHM data that contribute to this. (1) Lidar returns 
from short-statured vegetation are difficult to distinguish from the 
ground, so the &ldquo;ground&rdquo; estimated by Lidar is generally a bit higher 
than the true ground surface, and (2) the height estimate from Lidar 
represents the highest return, but the highest return may slightly 
miss the actual tallest point on a given tree. This is especially 
likely to happen with conifers, which are the top-of-canopy trees at 
Wind River.</p>

</body>

</html>
