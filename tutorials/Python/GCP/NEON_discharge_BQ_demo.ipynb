{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d82014d",
   "metadata": {},
   "source": [
    "# Query and analyze NEON discharge and water chemistry data via BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41bfea",
   "metadata": {},
   "source": [
    "Funded by the National Science Foundation (NSF) and proudly operated by Battelle, the [National Ecological Observatory Network (NEON)](https://www.neonscience.org/) program provides open, continental-scale data across the United States that characterize and quantify complex, rapidly changing ecological processes. The Observatoryâ€™s comprehensive design supports greater understanding of ecological change and enables forecasting of future ecological conditions. NEON collects and processes data from field sites located across the continental U.S., Puerto Rico, and Hawaii over a 30-year timeframe. NEON provides free and open data that characterize plants, animals, soil, nutrients, freshwater, and the atmosphere. These data may be combined with external datasets or data collected by individual researchers to support the study of continental-scale ecological change.\n",
    "\n",
    "This notebook is a demonstration of accessing and querying NEON data stored in Google Cloud BigQuery. Only a small subset of NEON data are currently available in this format, but more may be added in the future. The example datasets used are [Continuous discharge (DP4.00130.001)](https://data.neonscience.org/data-products/DP4.00130.001/RELEASE-2022) and [Chemical properties of surface water (DP1.20093.001)](https://data.neonscience.org/data-products/DP1.20093.001/RELEASE-2022). The RELEASE-2022 versions of both data products are available in BigQuery tables via the Analytics Hub. In both cases, we'll be focusing on the critical data table (csd_continuousDischarge and swc_externalLabDataByAnalyte, respectively) but other data and metadata tables are also available in the Analytics Hub listing. For more details about the two data products, see the product details pages at the links above, and the variables tables in each Analytics Hub listing.\n",
    "\n",
    "These two data products can be used together to estimate nutrient fluxes in stream ecosystems. In this notebook we first demonstrate a query on each data product independently, then a query that brings the two together to estimate dissolved organic carbon fluxes across NEON's stream sites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f75c01",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "#### Set up BigQuery account and authentication\n",
    "\n",
    "To access data in BigQuery, you will need a Google Cloud Platform account. When you first create an account, it will be assigned $300 in credit, to be used in the first 90 days. After that, GCP charges based on size of query and compute. The 90-day free trial can help you explore this - check your billing page, with the Promotions checkbox un-checked, and you can see the amount charged against your initial credit. When we set up this tutorial, all of the queries were below the free threshold, and the account was not charged at all.\n",
    "\n",
    "Account setup steps borrowed from [Getting started with BigQuery](https://colab.research.google.com/notebooks/bigquery.ipynb).\n",
    "\n",
    "1.   Use the [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to Create a Cloud Platform project if you do not already have one.\n",
    "2.   [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project.\n",
    "3.   [Enable BigQuery](https://console.cloud.google.com/flows/enableapi?apiid=bigquery) APIs for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5b50a",
   "metadata": {},
   "source": [
    "Provide account authentication to the Python instance in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aca127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "print('Authenticated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ea6a9",
   "metadata": {},
   "source": [
    "#### Subscribe to NEON datasets\n",
    "\n",
    "The two NEON datasets in Analytics Hub are shared with the public, but they are not part of the formal BigQuery Public Datasets program. This means you need to link them to your GCP project manually.\n",
    "\n",
    "Go to the Analytics Hub listing for [NEON Continuous discharge](https://console.cloud.google.com/bigquery/analytics-hub/exchanges/projects/304726359483/locations/us-central1/dataExchanges/National_Ecological_Observatory_Network_NEON_pilot/listings/neon_continuous_discharge) and click \"Add dataset to project\". This will open a window where you choose which project to link the dataset to, and the name of the dataset. Projects are specific to your account; use the project you prefer. The default name for the dataset will include disallowed characters; rename it to `neon_continuous_discharge`.\n",
    "\n",
    "Repeat this process for [NEON Chemical properties of surface water](https://console.cloud.google.com/bigquery/analytics-hub/exchanges/projects/304726359483/locations/us-central1/dataExchanges/National_Ecological_Observatory_Network_NEON_pilot/listings/neon_chemical_properties_of_surface_water). Use the same project for both datasets, and name the second dataset `neon_chemical_properties_of_surface_water`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb223bd",
   "metadata": {},
   "source": [
    "#### Import code libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083bb3d",
   "metadata": {},
   "source": [
    "#### Load BigQuery magic command capability\n",
    "\n",
    "Magic commands are a Jupyter feature, in this case they transform the entire code cell into one big SQL query. Learn more about magic commands in [IPython documentation](https://ipython.readthedocs.io/en/stable/interactive/magics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a69fc",
   "metadata": {},
   "source": [
    "### 2. Query Continuous discharge data\n",
    "\n",
    "Query for Continuous discharge (DP4.00130.001) data from the RELEASE-2022 dataset in BigQuery. We'll download the site, date, discharge rate, and the two-standard-deviation range for all data in RELEASE-2022, filtering out any records with the final quality flag raised, or where a discharge value is not populated.\n",
    "\n",
    "Syntax: `%%bigquery` is the magic command that designates the entire cell as one query command. `csd_core` is the name of the object to write the output to. `--project` specifies the project where the dataset is stored. You will need to change the text `your-project-id-here` to the project where you linked the NEON datasets.\n",
    "\n",
    "The query itself is written in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery csd_core --project your-project-id-here\n",
    "SELECT\n",
    "    siteID,\n",
    "    endDate,\n",
    "    maxpostDischarge,\n",
    "    withRemnUncQLower2Std,\n",
    "    withRemnUncQUpper2Std\n",
    "FROM\n",
    "    `neon_continuous_discharge.csd_continuousDischarge`\n",
    "WHERE\n",
    "    dischargeFinalQF = 0 AND \n",
    "    maxpostDischarge IS NOT NULL\n",
    "ORDER BY\n",
    "    siteID, endDate;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76160f57",
   "metadata": {},
   "source": [
    "Take a look at the first 10 rows to make sure the data look as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f855e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csd_core[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ea13a",
   "metadata": {},
   "source": [
    "The numeric data are imported from BigQuery as type \"decimal\", which `pandas` doesn't work with correctly. Convert the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c246294",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_core.maxpostDischarge = csd_core.maxpostDischarge.astype(\"float\")\n",
    "csd_core.withRemnUncQLower2Std = csd_core.withRemnUncQLower2Std.astype(\"float\")\n",
    "csd_core.withRemnUncQUpper2Std = csd_core.withRemnUncQUpper2Std.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff79e82",
   "metadata": {},
   "source": [
    "Start by subsetting to a single site, to explore the data before proceeding to larger analyses. Let's look at Como Creek (COMO), in Colorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ee634",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_COMO = csd_core[csd_core[\"siteID\"] == \"COMO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7480fc",
   "metadata": {},
   "source": [
    "Start by plotting the data for all time at COMO, including the upper and lower uncertainty bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(csd_COMO.endDate, \n",
    "         csd_COMO.maxpostDischarge,\n",
    "         linewidth=0.25, color=\"black\")\n",
    "plt.fill_between(csd_COMO.endDate, \n",
    "                 csd_COMO.withRemnUncQLower2Std, \n",
    "                 csd_COMO.withRemnUncQUpper2Std,\n",
    "                 color=\"gray\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c1f86",
   "metadata": {},
   "source": [
    "There are three years of data for COMO in RELEASE-2022. This stream is in the Rocky Mountains, and we can see clearly that there is a long period each winter when flow is extremely low or zero, with almost all the flow happening in a few months. NEON covers a very broad range of ecosystem types, so we expect to see very different patterns at different sites.\n",
    "\n",
    "### Query Chemical properties of surface water data\n",
    "\n",
    "Now let's get the water chemistry data. For this demo, we'll be focusing on dissolved organic carbon (DOC), so we can select just that analyte from the RELEASE-2022 dataset in BigQuery. Again, change `your-project-id-here` to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6239eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery swc_doc --project your-project-id-here\n",
    "SELECT\n",
    "    siteID,\n",
    "    collectDate,\n",
    "    analyteConcentration AS docConcentration\n",
    "FROM\n",
    "    `neon_chemical_properties_of_surface_water.swc_externalLabDataByAnalyte`\n",
    "WHERE\n",
    "    analyte = \"DOC\"\n",
    "ORDER BY\n",
    "    siteID, collectDate;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb5b3d",
   "metadata": {},
   "source": [
    "Again, check the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a3eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "swc_doc[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c790529",
   "metadata": {},
   "outputs": [],
   "source": [
    "swc_doc.docConcentration = swc_doc.docConcentration.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd9b24",
   "metadata": {},
   "source": [
    "And plot DOC concentration for COMO on the right-hand axis along with the discharge data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_COMO = swc_doc[swc_doc[\"siteID\"] == \"COMO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(csd_COMO.endDate, \n",
    "         csd_COMO.maxpostDischarge,\n",
    "         linewidth=0.25, color=\"black\")\n",
    "ax1.fill_between(csd_COMO.endDate, \n",
    "                 csd_COMO.withRemnUncQLower2Std, \n",
    "                 csd_COMO.withRemnUncQUpper2Std,\n",
    "                 color=\"gray\", alpha=0.2)\n",
    "ax2.scatter(doc_COMO.collectDate,\n",
    "            doc_COMO.docConcentration,\n",
    "            marker=\"o\", color=\"blue\", s=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd0346",
   "metadata": {},
   "source": [
    "We learn a couple of things from this figure: first, DOC concentrations rise and fall in parallel with discharge rate,  and second, there are chemistry samples collected from time periods when discharge estimates aren't available. Of course, since discharge estimates are made every minute, there are also many discharge estimates that don't have corresponding DOC samples.\n",
    "\n",
    "In order to calculate the flux of DOC, we need to do some alignment of the two datasets. The end result we want is a series of sites and dates, each with a corresponding discharge value and DOC value. To get there, we need to:\n",
    "* Subset the chemistry data to only the sites contained in the discharge data (no discharge at lake sites)\n",
    "* Match the dates between the chemistry and discharge data\n",
    "* Calculate a discharge value for the time period represented by each DOC collection\n",
    "\n",
    "### Query both data products together and join\n",
    "\n",
    "We could do this by transforming the `pandas` data frames we already have, and that would be a reasonable approach. But we're here to explore the capabilities of BigQuery. It enables us to calculate the average discharge on each day of each year and join the data to the DOC data from the days it was collected, using SQL. Using this approach, we only access the subsetted, aggregated data, rather than downloading the entirety of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery flux --project your-project-id-here\n",
    "SELECT\n",
    "*\n",
    "FROM\n",
    "    (SELECT\n",
    "        siteID,\n",
    "        EXTRACT(YEAR FROM endDate) AS yr,\n",
    "        EXTRACT(DAYOFYEAR FROM endDate) AS doy,\n",
    "        AVG(maxpostDischarge) AS dischargeMean\n",
    "    FROM\n",
    "        `neon_continuous_discharge.csd_continuousDischarge`\n",
    "    WHERE\n",
    "        dischargeFinalQF = 0 AND \n",
    "        maxpostDischarge IS NOT NULL\n",
    "    GROUP BY\n",
    "        siteID, \n",
    "        EXTRACT(YEAR FROM endDate),\n",
    "        EXTRACT(DAYOFYEAR FROM endDate)) csd\n",
    "JOIN\n",
    "    (SELECT\n",
    "        siteID,\n",
    "        EXTRACT(YEAR FROM collectDate) AS yr,\n",
    "        EXTRACT(DAYOFYEAR FROM collectDate) AS doy,\n",
    "        analyteConcentration AS doc\n",
    "    FROM\n",
    "        `neon_chemical_properties_of_surface_water.swc_externalLabDataByAnalyte`\n",
    "    WHERE\n",
    "        analyte = \"DOC\") swc\n",
    "ON\n",
    "    csd.siteID = swc.siteID AND\n",
    "    csd.yr = swc.yr AND\n",
    "    csd.doy = swc.doy\n",
    "ORDER BY\n",
    "    csd.siteID, csd.yr, csd.doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec4d07",
   "metadata": {},
   "source": [
    "The table `flux` contains one record for each day water chemistry was sampled, and the mean discharge rate from that day. Let's calculate the flux of DOC for each of those days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux[\"flux\"] = flux.dischargeMean*flux.doc\n",
    "flux.flux = flux.flux.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e54ff1",
   "metadata": {},
   "source": [
    "Now we have an estimate of the flux of DOC for a couple of dozen days per site per year. There are many directions an analysis could go at this point, let's pick something that can make a simple visualization. We'll find the maximum daily flux of DOC from each site across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_max = []\n",
    "for i in list(dict.fromkeys(flux.siteID)):\n",
    "    flux_max.append([i, max(flux.flux[flux[\"siteID\"] == i])])\n",
    "fluxmax = pandas.DataFrame(flux_max)\n",
    "fluxmax.columns = [\"siteID\",\"docFlux\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a894ed",
   "metadata": {},
   "source": [
    "Using site coordinates retrieved from NEON's public API, and the `Plotly Express` module, we can map maximum daily DOC flux at NEON stream sites across the country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "utmlist = []\n",
    "for i in fluxmax.siteID:\n",
    "    site_request = requests.get(\"https://data.neonscience.org/api/v0/locations/\" + i)\n",
    "    site_list = site_request.json()\n",
    "    utmlist.append([i, site_list[\"data\"][\"locationDecimalLatitude\"], \n",
    "                    site_list[\"data\"][\"locationDecimalLongitude\"]])\n",
    "utms = pandas.DataFrame(utmlist)\n",
    "utms.columns = [\"siteID\",\"lat\",\"lon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utms[\"docFlux\"] = fluxmax.docFlux/100\n",
    "docUtm = utms[pandas.notna(utms[\"docFlux\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c061ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(docUtm, lat=\"lat\", lon=\"lon\", \n",
    "                     size=\"docFlux\", scope=\"usa\",\n",
    "                     projection=\"albers usa\",\n",
    "                     color=\"siteID\", size_max=150,\n",
    "                     opacity=0.6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6b619",
   "metadata": {},
   "source": [
    "The two large rivers in the southeast dominate the map; let's take a look without them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamDoc = docUtm.drop(docUtm[(docUtm.siteID == 'BLWA') | (docUtm.siteID == 'FLNT')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(streamDoc, lat=\"lat\", lon=\"lon\", \n",
    "                     size=\"docFlux\", scope=\"usa\",\n",
    "                     projection=\"albers usa\",\n",
    "                     color=\"siteID\", size_max=50,\n",
    "                     opacity=0.6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf1283",
   "metadata": {},
   "source": [
    "We can see the distribution of maximum DOC flux across the country.\n",
    "\n",
    "### Final thoughts\n",
    "\n",
    "A few notes on this demo. This was a simple exercise for illustration purposes, not a rigorous analysis. We calculated the DOC flux on each day when DOC was sampled, but the continuous discharge data are available on a 1-minute resolution. More typical for this type of analysis would be to build a model estimating the flux as a function of discharge, and use the high-resolution discharge data to extrapolate a total flux estimate for the entire year.\n",
    "\n",
    "Similarly, there are many possible pathways to take regarding how much aggregation and data cleaning to carry out in the query step, as opposed to querying broadly and doing the data wrangling in the analysis phase. Choosing what to do in which part of the process depends on the nature of the analysis, and on familiarity with the data and data structures. In general, applying extensive data cleaning in the query step makes processing and analysis faster, by taking advantage of BigQuery's high speed performance and capabilities, but at the risk of missing critical nuance in the data. For example, if we didn't know to retain only the records where `dischargeFinalQF=0`, the daily mean discharge estimate from the query would include data that had been flagged as erroneous. As with all analyses, explore the data carefully before choosing an analytical approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21c290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
