{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e962dad7",
   "metadata": {},
   "source": [
    "---\n",
    "syncID: \n",
    "title: \"Detecting changes in vegetation structure following fires using discrete-return LiDAR\" \n",
    "description: \"Use discrete lidar point cloud and raster data to understand post-wildfire vegetation changes.\"\n",
    "dateCreated: 2023-08-24 \n",
    "authors: Shashi Konduri\n",
    "contributors: \n",
    "estimatedTime: 1 hour\n",
    "packagesLibraries: rasterio, rioxarray, laspy, pyproj, shapely, seaborn, geopandas\n",
    "topics: lidar, point cloud, \n",
    "languagesTool: python\n",
    "dataProduct: DP1.30003.001, DP3.30024.001\n",
    "code1: https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/Python/AOP/Lidar/lidar-applications/lidar-wildfire/fire-effects-veg-structure-lidar.ipynb\n",
    "tutorialSeries: \n",
    "urlTitle: discrete-lidar-wildfires\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c7817-cb5b-4a74-9cfd-ad763123f1af",
   "metadata": {},
   "source": [
    "The **Creek Fire** was a large wildfire that started in September 2020 in the Sierra National Forest, California and became one of the largest fires of the [2020 California wildfire season](https://en.wikipedia.org/wiki/2020_California_wildfires). This fire had burned into NEON's [Soaproot Saddle (SOAP)](https://www.neonscience.org/field-sites/soap) site by mid-September, causing a [high intensity burn over much of the site](https://www.neonscience.org/impact/observatory-blog/domain-digest-no-4-fire-field-sites-and-recovery) - standing trees became charcoal spires, shrubs and their root systems burned completely, the thick litter layer was incinerated, and the soil severely burned.\n",
    "\n",
    "The NEON Airborne Observation Platform (AOP) conducted aerial surveys over SOAP in 2019 and 2021, a year before and after the Creek Fire. This exercise aims to study the effects of fire on vegetation structure by comparing the lidar-derived relative height percentiles before and after the fire. In addition to the discrete return data, this tutorial uses a Digital Terrain Model (DTM) to determine the relative height of the discrete returns with respect to the ground.\n",
    "\n",
    "This Python tutorial is broken down into three parts:\n",
    "1. Read the NEON discrete-return lidar data ([DP1.30003.001](https://data.neonscience.org/data-products/DP1.30003.001)) and visualize the 3D lidar point cloud.\n",
    "2. Read the lidar-derived Digital Terrain Model ([DP3.30024.001](https://data.neonscience.org/data-products/DP3.30024.001)) in Python. Visualize the spatial extent of the lidar data used in this tutorial with that of the Creek Fire perimeter and the SOAP flight boundary.\n",
    "3. Calculate and compare the relative height percentiles of the discrete returns before and after the 2020 Creek Fire.\n",
    "\n",
    "NEON provides both discrete return and full-waveform LiDAR data for its field sites across the US. The discrete return LiDAR point cloud differs from the full waveform data as it is considerably smaller in file size and condenses the information into a small number of points per laser shot rather than many data bins (roughly 100 bins per laser shot in the full waveform data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eae7e9-1020-4ff0-9f34-a2e32eccbfb2",
   "metadata": {},
   "source": [
    "### Other NEON tutorials on the use of discrete-return lidar\n",
    "* If you're interested in using the NEON API to download lidar data using just a few lines of code, please refer to this [python tutorial](https://www.neonscience.org/resources/learning-hub/tutorials/neon-discrete-point-clouds).\n",
    "* For a more general indroduction to lidar and how to process lidar data in R, please follow this [R Tutorial series](https://www.neonscience.org/resources/learning-hub/tutorials/introduction-light-detection-and-ranging-lidar-explore-point#toggle-0). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6c84cdf-88b1-4789-be8c-04c133aa2959",
   "metadata": {},
   "source": [
    "## Required resources for this tutorial\n",
    "The tutorial pre-requisites, the IPython Notebook code, and the datasets used in this tutorial are available on this [Google Drive link](https://drive.google.com/drive/folders/1b-hU4ZgW32gMLzP3_qkv5H2PrXHcY2hL?usp=sharing). The pdf file on tutorial prerequisites goes over the instructions for installing the Anaconda distribution of Python and QGIS. You could also run this tutorial on Google Colab if you do not wish to install Python on your local machine. The subfolder named \"NEON_lidar_tutorial_datasets\" has shapefiles for the 2020 Creek Fire perimeter, a kml file for the Soaproot site boundary, discrete-return lidar data for the years 2019 and 2021 in .laz format for a small fire-affected area within the SOAP site, and DTM data in .tif format for the same location. The unzipped folder is about 114 MB in size. The shapefile for the 2020 Creek Fire perimeter used in this tutorial was downloaded from the California Department of Forestry and Fire Protection (CAL FIRE) [website](https://hub-calfire-forestry.hub.arcgis.com/datasets/CALFIRE-Forestry::california-fire-perimeters-all/explore?filters=eyJZRUFSXyI6WyIyMDIwIl0sIkZJUkVfTkFNRSI6WyJDUkVFSyJdfQ%3D%3D&layer=1&location=37.228068%2C-119.146292%2C9.84). It takes about 10 - 15 minutes to execute the code in this Jupyter Notebook depending on whether you run the code locally on your machine or on Google Colab.\n",
    "\n",
    "### Starting the Jupyter Notebook on your local machine\n",
    "You can launch the Jupyter Notebook by going to the Start Menu and selecting \"Jupyter Notebook\" under \"Anaconda3\". Another way is to open a terminal window and run the command \"jupyter notebook\". This will instantiate a local server on your browser. Click on the \"New\" dropdown list in the top right corner and select Notebook. This should start an IPython Notebook kernel. \n",
    "\n",
    "### Starting a Google Colab instance\n",
    "If you prefer using [Colab](https://colab.google/) instead, click on \"New Notebook\". You will have to login to your Google Account. Once the login is successful, it should launch a Colab instance. Every Google Colab instance provides 12 GB RAM usage for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ba387-f5a8-4911-88a6-9bd17a9d2a67",
   "metadata": {},
   "source": [
    "## Set up the working directory\n",
    "First, download the [Google Drive folder](https://drive.google.com/drive/folders/1b-hU4ZgW32gMLzP3_qkv5H2PrXHcY2hL?usp=sharing) to your local machine by clicking on \"Download all\", as shown in the [screenshot](https://drive.google.com/file/d/1P7dwmevP4iJi3WFD7qKM7Pby7nCmRtkw/view?usp=drive_link). This will download a zipped file that must be extracted. **Make sure the extracted folder's name is \"NEON_lidar_tutorial\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7dbb3b-4e3f-42fd-8aab-6769cb66c048",
   "metadata": {},
   "source": [
    "### 1. If you're using Jupyter Notebook on local machine\n",
    "The code snippet in the cell below will set the working directory as \"NEON_lidar_tutorial_datasets\" in the Downloads directory. You can execute a code cell by pressing the \"Shift + Enter\" keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4da733-336e-490e-a1f4-4020d3d62176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "downloads_path = home + os.sep + \"Downloads\"\n",
    "data_root_dir = os.path.join(downloads_path, \"NEON_lidar_tutorial\", \"NEON_lidar_tutorial_datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0c0a8-ad6e-4d17-a881-204d363a68a8",
   "metadata": {},
   "source": [
    "**Note**: If you move the data to a different location, you must change the data root directory variable (`data_root_dir`) to the new directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb111fbf-68ee-48df-a34c-722dd39a9082",
   "metadata": {},
   "source": [
    "### 2. If you're using Google Colab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e366541-46b0-446e-951c-55e1ae7ea846",
   "metadata": {},
   "source": [
    "If you are using [Google Colab](https://research.google.com/colaboratory/), you can upload the Jupyter Notebook code (fire_effects_on_veg_structure_using_lidar.ipynb) stored locally in the \"NEON_lidar_tutorial\" folder by selecting \"File\" --> \"Upload Notebook\", as shown in the [screenshot](https://drive.google.com/file/d/1h5Y1lXN4NdFm7zXbUbm3KQGtfm6uoBhI/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1d5e5-aa35-461b-850e-c3cc4e191fa7",
   "metadata": {},
   "source": [
    "In order to access your datasets from Colab, you can first upload the \"NEON_lidar_tutorial\" folder to your Google Drive. You can then mount the Google Drive on Colab by clicking on the \"Files\" icon on the left followed by clicking on the \"Mount Drive\" icon, as shown in this [screenshot](https://drive.google.com/file/d/1TFefnrlnpnVNJRgRzG5sUe-F2RDUKC1q/view?usp=drive_link).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130039f-dac5-40bf-b2de-eda25c1e9112",
   "metadata": {},
   "source": [
    "**Note**: When using Colab, the `data_root_dir` defined above would be different. Once you have uploaded the \"NEON_lidar_tutorial\" folder to your Google Drive and mounted the drive on Colab, you can walk through the directories to get to \"NEON_lidar_tutorial_datasets\", as shown in this [screenshot](https://drive.google.com/file/d/1-RDl2qgfj0cdKVrgg_Vp2qOIYKA41naw/view?usp=drive_link). You can get the path for this folder by right clicking on the \"NEON_lidar_tutorial_datasets\" folder --> \"Copy Path\" and set the `data_root_dir` variable to that path. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427335a4-0cb2-4996-ad91-d4bac564694b",
   "metadata": {},
   "source": [
    "If you are using Colab, uncomment the cell below and run it. Make sure that the path to \"NEON_lidar_tutorial_datasets\" is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156185ba-9c1d-44f0-b230-23573586a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_root_dir = \"/content/drive/MyDrive/NEON_lidar_tutorial/NEON_lidar_tutorial_datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fba58-db66-4c15-8495-4758850a5e59",
   "metadata": {},
   "source": [
    "## Install Python packages\n",
    "\n",
    "### Packages used \n",
    "* **rasterio & rioxarray** for reading and plotting raster data\n",
    "* **geopandas** for reading shapefiles and kml \n",
    "* **laspy** for reading lidar point cloud\n",
    "* **pyproj** to change map projections \n",
    "* **shapely** to create vector polygons\n",
    "* **seaborn** for making boxplots and histograms\n",
    "\n",
    "To install each of these Python packages from within the Jupyter Notebook, run the command \"!pip install\" followed by the package's name. Installing all the packages should typically take less than a minute. In a Jupyter Notebook environment, the exclamation mark (!) allows users to run shell commands inside the cell. You'll have to run the following cell only once to install the packages onto your machine. You can suppress the output of a code cell by right clicking on the code cell followed by clicking on \"Clear Cell Output\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b99d62-4974-4717-8ab9-e459c848763e",
   "metadata": {},
   "source": [
    "```\n",
    "!pip install rasterio\n",
    "!pip install rioxarray\n",
    "!pip install laspy[lazrs,laszip]\n",
    "!pip install pyproj\n",
    "!pip install shapely\n",
    "!pip install seaborn\n",
    "!pip install geopandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75291a67-9819-4b51-b38b-d6e8769d2d12",
   "metadata": {},
   "source": [
    "Now that the packages have been installed, let us import them into the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import laspy, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import pyproj\n",
    "from pyproj import Proj\n",
    "import shapely\n",
    "from shapely import Polygon, MultiPolygon  ## Try this if this line throws an error: from shapely.geometry import Polygon, MultiPolygon\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from rasterio.plot import show\n",
    "import random\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcec65",
   "metadata": {},
   "source": [
    "## Part 1: Reading and visualizing discrete return lidar data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa13165",
   "metadata": {},
   "source": [
    "### Read las files in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb0367",
   "metadata": {},
   "source": [
    "Set \"Discrete_return_lidar_returns\" as the working folder and import the point cloud data for 2019 and 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b134c89-b5d4-40e4-99dd-e430733ad248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discrete_lidar_directory and change into that directory to read in the point cloud files\n",
    "discrete_lidar_dir = os.path.join(data_root_dir,\"Discrete_return_lidar_returns\")\n",
    "os.chdir(discrete_lidar_dir)\n",
    "\n",
    "las_2021 = laspy.read(\"NEON_D17_SOAP_DP1_293000_4100000_classified_point_cloud_colorized_2021.laz\")\n",
    "las_2019 = laspy.read(\"NEON_D17_SOAP_DP1_293000_4100000_classified_point_cloud_colorized_2019.laz\")\n",
    "\n",
    "## Print all the fields provided by the las files\n",
    "print(list(las_2021.point_format.dimension_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e964cc",
   "metadata": {},
   "source": [
    "### Read point cloud data as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed43126-a13d-43a5-baad-b75d75711db3",
   "metadata": {},
   "source": [
    "The \"las_to_df\" function defined below imports the .las file as a Python dataframe. For more information about the las format, please refer to the [ASPRS documentation](https://www.asprs.org/wp-content/uploads/2010/12/LAS_1_4_r13.pdf). Las files provide the following information:\n",
    " * **X, Y, and Z (height above vertical datum) coordinates** of the discrete return.\n",
    "* **Intensity of the return**. In discrete-return lidar, laser pulses that reflect off single return objects, such as the ground have a much higher intensity than pulses that must penetrate the canopy (which lose photon energy with each interaction).\n",
    " * **Return number and total number of returns per pulse**. Each lidar pulse can have multiple returns along the vertical profile. The first return typically comes from the top of the canopy, while the last return tends to be from the ground.\n",
    " * **Classification of the return**. Lastools software follows the [standard ASPRS classification](https://www.asprs.org/wp-content/uploads/2010/12/LAS_1_4_r13.pdf) scheme to classify returns as ground, low vegetation, high vegetation, buildings, etc.\n",
    " * **Scan angle** of the lidar sensor at which the return was collected (can vary from -18 degrees to + 18 degrees). This information could be used to drop returns collected at extreme values of scan angle.\n",
    " \n",
    "For more information about how the NEON lidar point cloud data were created, refer to the [NEON L0-to-L1 Discrete Return LiDAR Algorithm Theoretical Basis Document (ATBD)](https://data.neonscience.org/api/v0/documents/NEON.DOC.001292vB) found on the [NEON data product page](https://data.neonscience.org/data-products/DP1.30003.001). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to import las file as a dataframe in python\n",
    "def las_to_df(las):\n",
    "  x = pd.DataFrame(np.array(las.x))\n",
    "  y = pd.DataFrame(np.array(las.y))\n",
    "  z = pd.DataFrame(np.array(las.z))\n",
    "  intensity = pd.DataFrame(np.array(las.intensity))\n",
    "  return_num = pd.DataFrame(np.array(las.return_number))\n",
    "  number_of_returns = pd.DataFrame(np.array(las.number_of_returns))\n",
    "  classification = pd.DataFrame(np.array(las.classification)) ## 0 - 31 as per ASPRS classification scheme\n",
    "    \n",
    "  df = pd.concat([x, y, z, intensity, return_num, number_of_returns, classification], axis=1)\n",
    "  df.columns=[\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\"]\n",
    "  return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the las_to_df function to extract the 2019 and 2021 data as separate dataframes\n",
    "point_cloud_df_2019 = las_to_df(las_2019)\n",
    "point_cloud_df_2021 = las_to_df(las_2021)\n",
    "point_cloud_df_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfce347",
   "metadata": {},
   "source": [
    "The X and Y output coordinates are reported as Easting and Northing values in a Universal Transverse Mercator (UTM) projection with the World Geodetic System 1984 (WGS84) International Terrestrial Reference Frame 2000 (ITRF 2000) ellipsoid horizontal datum with units of meters. The UTM zone will vary depending on the latitude/longitude of the specific NEON site. The Z coordinates are reported in a North American Vertical Datum 1988 (NAVD88) using the National Geodetic Survey Geoid12A height model with units of meters. For a gentle introduction to reference datums and projection systems, please refer to this [article](https://8thlight.com/insights/geographic-coordinate-systems-101).  \n",
    "\n",
    "For the Z values to make more sense, we will use the ground elevations from a Digital Terrain Model (DTM) to calculate the height of the discrete returns relative to the ground. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ec375",
   "metadata": {},
   "source": [
    "### Create 3D visualization of discrete-return point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06ffcaf",
   "metadata": {},
   "source": [
    "Before we get to calculating the heights of each return with respect to ground, we'll visualize the discrete returns in three dimensions. To visualize the 3D point cloud data, we first combine the X, Y, and Z dimensions using the numpy package (abbreviated here as np). We then plot the X, Y, and Z values for each discrete return and colorize each return with its R, G, and B values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54584387",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use np.stack to combine the X, Y and Z into one array\n",
    "## Convert (3 x n) array to (n x 3) using transpose\n",
    "point_data_2021 = np.stack([las_2021.X, las_2021.Y, las_2021.Z]).transpose()\n",
    "point_data_2019 = np.stack([las_2019.X, las_2019.Y, las_2019.Z]).transpose()\n",
    "print(\"Number of discrete returns in the 2021 point cloud file = %s\" %\"{:,}\".format(point_data_2021.shape[0]))\n",
    "print(\"Number of discrete returns in the 2019 point cloud file = %s\" %\"{:,}\".format(point_data_2019.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739d860-39ea-4444-adf0-c11791bc04be",
   "metadata": {},
   "source": [
    "Since the point cloud files have millions of discrete returns, plotting them will take a substantial amount of time (~15-20 minutes). To avoid spending a lot of time, we will sample a small fraction of them. In the code below, we will sample every 100th return for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c96d6a-4f06-465d-9b93-a0d7fd9fb472",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=100\n",
    "point_data_2021_sub = point_data_2021[::factor]\n",
    "point_data_2019_sub = point_data_2019[::factor]\n",
    "print(\"Number of discrete returns in the 2021 point cloud subsample = %s\" %\"{:,}\".format(point_data_2021_sub.shape[0]))\n",
    "print(\"Number of discrete returns in the 2019 point cloud subsample = %s\" %\"{:,}\".format(point_data_2019_sub.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5c9ea-5e41-46ec-9ed0-41bd5c6ffe4a",
   "metadata": {},
   "source": [
    "Now, we will extract the Red (R), Green (G), and Blue (B) values associated with each discrete return. The RGB data is collected by the camera sensor fitted on the airplane. Think of this as \"draping\" the camera imagery on top of the lidar discrete returns, so we have additional context when plotting the returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850995a-06c6-47ad-9c6d-1b64f63a0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_2021 = np.stack([las_2021.red, las_2021.green, las_2021.blue]).transpose()\n",
    "colors_2019 = np.stack([las_2019.red, las_2019.green, las_2019.blue]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6f4f8-48be-45ac-9404-3b908bd959c2",
   "metadata": {},
   "source": [
    "The RGB values are coded on 16 bits in the las file, ranging between $2^0$ and $2^{16}$ for each band. We need to scale the RGB color values to lie between 0 and 1 for plotting, thus, we are going to subtract the min(band value) and divide by the range of band values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10755985-f1bc-4fa7-9d75-820c4024083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_2021_normalized = (colors_2021 - np.min(colors_2021))/np.ptp(colors_2021)\n",
    "colors_2021_normalized_sub = colors_2021_normalized[::factor]\n",
    "## We are going to zip the R,G,B values together for plotting using the matplotlib package\n",
    "colors_2021_normalized_sub = list(zip(colors_2021_normalized_sub[:,0], colors_2021_normalized_sub[:,1], colors_2021_normalized_sub[:,2]))\n",
    "\n",
    "colors_2019_normalized = (colors_2019 - np.min(colors_2019))/np.ptp(colors_2019)\n",
    "colors_2019_normalized_sub = colors_2019_normalized[::factor]\n",
    "colors_2019_normalized_sub = list(zip(colors_2019_normalized_sub[:,0], colors_2019_normalized_sub[:,1], colors_2019_normalized_sub[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b69cd4-9a77-4738-959b-7807d158b4d8",
   "metadata": {},
   "source": [
    "Now, we will plot the x, y, and z coordinates of every discrete return sampled from 2021 and colorize them using the RGB value from the camera sensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d48f3-bb1a-4ee2-9db8-265dfffd8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the las data in 3D\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "## Plot the 2021 data\n",
    "ax.scatter(point_data_2021_sub[:,0], point_data_2021_sub[:,1], point_data_2021_sub[:,2], color=colors_2021_normalized_sub, s=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc36b73-0c8f-41c2-bbe3-9549a4d08119",
   "metadata": {},
   "source": [
    "The dark shade in the middle is a cloud shadow. We will now make a similar plot for returns sampled from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e461311-a9e8-42d5-b57a-2548605005a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(point_data_2019_sub[:,0], point_data_2019_sub[:,1], point_data_2019_sub[:,2], color=colors_2019_normalized_sub, s=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2999a",
   "metadata": {},
   "source": [
    "Visually comparing the randomly-sampled colorized point clouds from the two years reveals burn scars with gaps in vegetation in 2021 relative to 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201044d2",
   "metadata": {},
   "source": [
    "## Part 2: Ingest and visualize the Digital Terrain Model (DTM)\n",
    "\n",
    "Next, we'll explore a Python library called `rioxarray` for reading and plotting raster data. The rioxrray object has metadata information stored in the tiff tags. The DTM files are provided as 1 x 1 km tiles at a 1-m spatial resolution, which is why the resulting dataframe is of size 1000 x 1000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9926ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(data_root_dir,\"Digital_Terrain_Model\"))\n",
    "dtm_2021 = rioxarray.open_rasterio(\"NEON_D17_SOAP_DP3_293000_4100000_DTM_2021.tif\")\n",
    "dtm_2019 = rioxarray.open_rasterio(\"NEON_D17_SOAP_DP3_293000_4100000_DTM_2019.tif\")\n",
    "print(dtm_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b8034-9db5-4d0d-828f-477aaa350e83",
   "metadata": {},
   "source": [
    "You can expand the information stored in the specific attributes by using the 'attrs' keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f4749-b9f0-4265-858d-9c84b71c9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****2021 DTM Metdata******\")\n",
    "print(dtm_2021.attrs[\"TIFFTAG_IMAGEDESCRIPTION\"])\n",
    "\n",
    "print(\"****2019 DTM Metdata******\")\n",
    "print(dtm_2019.attrs[\"TIFFTAG_IMAGEDESCRIPTION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21e7b4",
   "metadata": {},
   "source": [
    "As per the information provided in the 'TIFFTAG_IMAGEDESCRIPTION' field in the metadata, it can be seen that NEON flew the older Optech Gemini lidar sensor in 2019 and the new Optech Galaxy Prime sensor in 2021. When comparing lidar data collected across different years, it is essential to check if the lidar sensors used for the collections are consistent. Older sensors like the Optech Gemini have a wider outgoing pulse width, resulting in poorer range resolution than the newer Optech Galaxy Prime. **Poor range resolution for a lidar sensor makes it challenging to resolve objects close to the ground (such as low vegetation).** Optech Gemini has a range resolution of about 2 m, which means it can be challenging to distinguish objects less than 2 m apart along the vertical profile. The range resolution for the newer Optech Galaxy Prime is substantially better, at around 67 cm. For more information on range resolution for NEON lidar sensors, please refer to section 4.1.1.3 of the [NEON Algorithm Theoretical Basis Document (ATBD)](https://data.neonscience.org/api/v0/documents/NEON.DOC.001292vB) for the discrete-return data. We can extract the ground elevations form the DTM raster as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6347f-28a1-4788-abd9-6f4aef24eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the DTM data, which is stored in band 1\n",
    "dtm_2021_data = dtm_2021.data[0,:,:]\n",
    "\n",
    "## Print the values stored in the DTM dataframe\n",
    "print(dtm_2021_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe2a5b-41d5-4249-ae38-b8ab0a84147f",
   "metadata": {},
   "source": [
    "The values stored in the DTM dataframe are ground elevations (in m) with respect to the NAVD88 reference datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_2021.plot()\n",
    "plt.title(\"2021 Digital Terrain Model showing ground elevations (in m) with respect to the \\n NAVD88 reference datum in UTM Zone 11N projection\", fontsize=10, pad=12)\n",
    "plt.ylabel(\"Northing (m)\", labelpad=12)\n",
    "plt.xlabel(\"Easting (m)\", labelpad=12)\n",
    "\n",
    "xticks = np.arange(293000, 294200, 200)\n",
    "comma_sep_xticks = []\n",
    "for num in xticks:\n",
    "   comma_sep_num = \"{:,}\".format(num)\n",
    "   comma_sep_xticks.append(comma_sep_num)\n",
    "\n",
    "yticks = np.arange(4100000,4101200,200)\n",
    "comma_sep_yticks = []\n",
    "for num in yticks:\n",
    "   comma_sep_num = \"{:,}\".format(num)\n",
    "   comma_sep_yticks.append(comma_sep_num)\n",
    "    \n",
    "plt.xticks(xticks, comma_sep_xticks)\n",
    "plt.yticks(yticks, comma_sep_yticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7d2c6",
   "metadata": {},
   "source": [
    "### Optional: Visualize the relative spatial extent of the lidar data used in relation to the Creek Fire perimeter\n",
    "\n",
    "You can skip this section and go straight to Part 3, as it is not key to this tutorial. Here, we will use the Python packages `geopandas` for ingesting vector data and `matplotlib` to plot the spatial extent of the 2020 Creek fire relative to that of the NEON site at Soaproot Saddle (SOAP). `Geopandas` is a popular Python package for handling vector data, such as shapefiles and kml files. The SOAP boundary file is provided in kml format, whereas the Creek fire extent is available as a shapefile. You will also notice that these files are available in different projections, which prevents us from plotting them on the same axis. Projection information is usually provided as a unique four-digit EPSG code and can be accessed from a Geopandas object using the keyword \"crs\". You may refer to this [article](https://8thlight.com/insights/geographic-coordinate-systems-101) again for more information on EPSG codes 3857 and 4326."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc418c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Go inside the creek fire boundary subfolder and ingest the shape file\n",
    "os.chdir(os.path.join(data_root_dir,\"Creek_fire_boundary\"))\n",
    "creek_fire_perimeter = gpd.read_file(\"creek_fire_perimeter.shp\")\n",
    "creek_fire_perimeter.plot(color='red', figsize=(3.5,5))\n",
    "plt.ylabel(\"Northing\", labelpad=12)\n",
    "plt.xlabel(\"Easting\", labelpad=12)\n",
    "plt.show()\n",
    "print(\"Projection for creek fire shapefile = %s\" %creek_fire_perimeter.crs)\n",
    "\n",
    "## Now go into the Soaproot site boundary subfolder and ingest the kml file\n",
    "os.chdir(os.path.join(data_root_dir,\"SOAP_site_boundary\"))\n",
    "#gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'  ## you might need this to enable geopandas to read a kml file\n",
    "soap_boundary = gpd.read_file(\"NEON_D17_SOAP_DPQA_2021_full_boundary.kml\", driver='KML')\n",
    "soap_boundary.plot(color=\"yellow\", figsize=(3,3))\n",
    "plt.ylabel(\"Latitude\", labelpad=12)\n",
    "plt.xlabel(\"Longitude\", labelpad=12)\n",
    "plt.show()\n",
    "print(\"Projection for Soaproot Saddle kml file = %s\" %soap_boundary.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b9b72-5a7a-4dad-bd3d-f9796d4d2eea",
   "metadata": {},
   "source": [
    "Maps are usually provided in different projections, and converting from one to another is common. Here, we will convert the Creek Fire shapefile from EPSG:3857 to 4326 using the `Pyproj` package. Since the Creek fire shapefile is a collection of polygons, the coordinates for each of the polygon need to converted from EPSG:3857 to 4326. The following code cell demonstrates how this conversion works for one polygon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188a6e7-38b2-4047-9870-b660c68fe680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the Creek fire polygons as a list\n",
    "multipolygons = list(creek_fire_perimeter[\"geometry\"][0].geoms)\n",
    "\n",
    "## Print the first polygon from the multi polygons list\n",
    "index=0\n",
    "print(multipolygons[index])\n",
    "\n",
    "## Extract the x and y for the first polygon\n",
    "y = list(multipolygons[index].exterior.coords.xy[0])\n",
    "x = list(multipolygons[index].exterior.coords.xy[1])\n",
    "\n",
    "## Create projection objects for the source and target projections \n",
    "epsg_3857 = Proj(projparams = 'epsg:3857')\n",
    "epsg_4326 = Proj(projparams = 'epsg:4326')\n",
    "\n",
    "## Transform coordinates from EPSG:3857 to EPSG:4326\n",
    "latlons = Polygon(list(zip(pyproj.transform(epsg_3857, epsg_4326, y, x)[0], pyproj.transform(epsg_3857, epsg_4326, y, x)[1])))\n",
    "print(latlons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183225dd-eb22-4342-8364-d60107bab075",
   "metadata": {},
   "source": [
    "We will now run a for loop to reproject the coordinates of all polygons in the creek fire shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec0aaa-b9dc-4a7c-b57f-83bb35f3fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipolygon_latlon_list = []\n",
    "\n",
    "for index in range(0, len(multipolygons)):\n",
    "    ## Extract the x and y for the polygon\n",
    "    y = list(multipolygons[index].exterior.coords.xy[0])\n",
    "    x = list(multipolygons[index].exterior.coords.xy[1])\n",
    "    multipolygon_latlon_list.append(Polygon(list(zip(pyproj.transform(epsg_3857, epsg_4326, y, x)[1], pyproj.transform(epsg_3857, epsg_4326, y, x)[0]))))\n",
    "\n",
    "## Update the geometry information in the creek fire perimeter \n",
    "creek_fire_perimeter[\"geometry\"][0] = MultiPolygon(multipolygon_latlon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7f89e3",
   "metadata": {},
   "source": [
    "We can also plot the extent of 2021 DTM, which is available in yet another projection, UTM Zone 11N. We will again use the `Pyproj` package to convert the UTM coordinates of the DTM bounding box to lat lon. Then we will use the `Shapely` package to create a polygon whose corners are the lat/lon values of the DTM bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b5317-3b10-4cf4-8a36-2de70052de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(data_root_dir,\"Digital_Terrain_Model\"))\n",
    "dtm_2021 = rasterio.open(\"NEON_D17_SOAP_DP3_293000_4100000_DTM_2021.tif\")\n",
    "dtm_2019 = rasterio.open(\"NEON_D17_SOAP_DP3_293000_4100000_DTM_2019.tif\")\n",
    "\n",
    "## Convert UTM coordinates of the DTM bounding box to lat/lon\n",
    "utm_proj = Proj(\"+proj=utm +zone=11 +north +ellps=WGS84 +datum=WGS84 +units=m\")\n",
    "lon_min, lat_min = utm_proj(dtm_2021.bounds[0], dtm_2021.bounds[1], inverse=True)\n",
    "lon_max, lat_max = utm_proj(dtm_2021.bounds[2], dtm_2021.bounds[3], inverse=True)\n",
    "\n",
    "corner_coords = [[lon_min, lat_min], [lon_min, lat_max], [lon_max, lat_max], [lon_max, lat_min]]\n",
    "\n",
    "## Create a shapely polygon using corner lat/lons\n",
    "polygon_geometry = Polygon(corner_coords)\n",
    "df = {'Attribute':['DTM_extent'], 'geometry':polygon_geometry} # create a dictionary with needed attributes\n",
    "dtm_polygon = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884bc4a-2814-45a0-9aa0-656f6eb7e88e",
   "metadata": {},
   "source": [
    "Let's plot the spatial extent of the creek fire, Soaproot saddle site (SOAP) boundary, and the 1 x 1 km 2021 DTM raster together, now that they are all in lat/lon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c3f67-6e5a-47d4-bd1a-7d7287b772ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "creek_fire = creek_fire_perimeter.plot(color='red', figsize=(5,5))\n",
    "soap_boundary.plot(ax=creek_fire, color='yellow')\n",
    "dtm_polygon.plot(ax=creek_fire, color='black')\n",
    "plt.ylabel(\"Latitude\", labelpad=12)\n",
    "plt.xlabel(\"Longitude\", labelpad=12)\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "## Add legend\n",
    "legend_elements = [Patch(facecolor='red', edgecolor='red', label='2020 Creek Fire perimeter'), \n",
    "                  Patch(facecolor='none', edgecolor='yellow', label='Total extent of lidar data collected for SOAP site'),\n",
    "                  Patch(facecolor='black', edgecolor='black', label='Extent of the lidar data used in this tutorial')]\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(2.6,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e8544",
   "metadata": {},
   "source": [
    "## Part 3: Calculate and compare the relative height percentiles of the discrete returns before (2019) and after (2021) the 2020 Creek fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a95d41",
   "metadata": {},
   "source": [
    "We will now be calculating pixel-wise height percentiles ($20^{th}$, $30^{th}$, $50^{th}$, $75^{th}$, and $90^{th}$ percentile heights) of the discrete returns relative to the ground (DTM) for the years 2019 and 2021. To ensure that we have sufficient number of lidar returns per pixel for calculating percentiles, we will be calculating these metrics for every 10m pixel on the ground. So, our tasks here would be to:\n",
    "\n",
    "1) Calculate heights of discrete returns with repect to the 1m DTM. Since the ground heights do not change from year to year, we will be using the 2021 DTM to calculate heights of the discrete returns with respect to ground for both the years 2019 and 2021. The 2020 fire event would have likely cleared up some of the low vegetation and ground litter, thereby improving ground detection post-fire in 2021. Therefore, the 2021 DTM may be a more accurate representation of the true ground compared to the 2019 DTM.\n",
    "2) Create a 10m spatial resolution raster grid and assign a unique id to each 10m pixel.\n",
    "3) Group all discrete returns based on the 10m pixel they fall into.\n",
    "4) Calculate height percentiles ($20^{th}$, $30^{th}$, $50^{th}$, $75^{th}$, and $90^{th}$ percentile heights) relative to the ground for each 10m pixel.\n",
    "5) Compare the distribution of heights for each height percentile before and after the fire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fe48bf",
   "metadata": {},
   "source": [
    "### 1. Discrete-return heights relative to 2021 DTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00476ce1",
   "metadata": {},
   "source": [
    "To extract the ground elevation associated with each discrete return, we will be sampling values from the 2021 DTM raster at the x,y locations of each discrete return. We can then subtract the ground elevation from the height of the discrete return (z) to get the height of the return above ground. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zip all x and y coordinates together for sampling rasters in the next cell\n",
    "coords_2019 = [(x,y) for x, y in zip(point_cloud_df_2019[\"x\"], point_cloud_df_2019[\"y\"])]\n",
    "coords_2021 = [(x,y) for x, y in zip(point_cloud_df_2021[\"x\"], point_cloud_df_2021[\"y\"])]\n",
    "\n",
    "print(\"Number of discrete returns in 2021 data = %d\" %len(coords_2021))\n",
    "print(\"Number of discrete returns in 2019 data = %d\" %len(coords_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff10005-5a0f-42da-83d6-9bb4f33caf24",
   "metadata": {},
   "source": [
    "As you can see, there are about 15 million discrete returns in the 2021 point cloud data compared to the roughly 3 million returns in the 2019 data. The difference in the # of returns can be attributed to the different sensors used in the two years. Sampling the ground elevations for all these returns would take several minutes. In the next cell, we are going to sample 1 million returns randomly for 2019 and 2021. Extracting the ground elevations for these million returns for both years would take about 2 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c973ac-72f4-46b4-8031-4f8de3558b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set a seed value to make the code output reproducible\n",
    "seed_val = 0\n",
    "random.seed(seed_val)\n",
    "\n",
    "## Randomly sample 1 million rows from the 2019 and 2021 point cloud dataframes\n",
    "point_cloud_df_2019_sub = point_cloud_df_2019.sample(n=1000000).reset_index(drop=True)\n",
    "point_cloud_df_2021_sub = point_cloud_df_2021.sample(n=1000000).reset_index(drop=True)\n",
    "\n",
    "## zip all x and y coordinates together for sampling rasters in the next step\n",
    "coords_2019_sub = [(x,y) for x, y in zip(point_cloud_df_2019_sub[\"x\"], point_cloud_df_2019_sub[\"y\"])]\n",
    "coords_2021_sub = [(x,y) for x, y in zip(point_cloud_df_2021_sub[\"x\"], point_cloud_df_2021_sub[\"y\"])]\n",
    "\n",
    "## Sample the raster using \"rasterio.sample.sample_gen()\"\n",
    "\n",
    "## Sample the 2021 DTM raster for ground elevation at each (x,y) location of the 2021 discrete returns\n",
    "dtm_vals_2021 = pd.DataFrame(list(rasterio.sample.sample_gen(dtm_2021, coords_2021_sub)))\n",
    "\n",
    "## Sample the 2021 DTM raster for ground elevation at each (x,y) location of the 2019 discrete returns\n",
    "## The ground elevations shouldn't change from year to year, so it is OK to use the 2021 DTM for both years.\n",
    "## Moreover, the fire event in 2020 likely cleared the understory vegetation, and therefore, the ground elevations might be more accurate in 2021\n",
    "dtm_vals_2019 = pd.DataFrame(list(rasterio.sample.sample_gen(dtm_2021, coords_2019_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fced7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the point cloud dataframe (point_cloud_df_20xx_sub) with the DTM ground elevations extracted (dtm_vals_20xx)\n",
    "## Calculate the relative height of each return with respect to ground (\"discrete_ret_ht_above_ground\") \n",
    "\n",
    "## 2019\n",
    "df_2019 = pd.concat([point_cloud_df_2019_sub, dtm_vals_2019], axis=1)\n",
    "df_2019.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \"ground_elevation\"]\n",
    "df_2019[\"discrete_ret_ht_above_ground\"] = df_2019[\"z\"] - df_2019[\"ground_elevation\"]\n",
    "\n",
    "## Do the same for 2021 data as well\n",
    "df_2021 = pd.concat([point_cloud_df_2021_sub, dtm_vals_2021], axis=1)\n",
    "df_2021.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \"ground_elevation\"]\n",
    "df_2021[\"discrete_ret_ht_above_ground\"] = df_2021[\"z\"] - df_2021[\"ground_elevation\"]\n",
    "df_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54e1a1",
   "metadata": {},
   "source": [
    "### 2. Create a 10m spatial resolution raster grid and assign a unique id to each 10m pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the new 5m raster in the DTM data folder\n",
    "dtm_directory = os.path.join(data_root_dir,\"Digital_Terrain_Model\")\n",
    "os.chdir(dtm_directory)\n",
    "\n",
    "## Create a (100 x 100) dataframe with each cell having a unique value between 0 and 10,000  \n",
    "ids = pd.DataFrame(data=np.arange(0, 100*100).reshape(100,100), index=np.arange(0,100), columns=np.arange(0,100))\n",
    "\n",
    "## Create a scaled transform\n",
    "scaled_transform = dtm_2021.transform * dtm_2021.transform.scale((10),(10))\n",
    "\n",
    "## Using rasterio, save the dataframe as a 10m raster in the tif format \n",
    "with rasterio.open(\n",
    "    dtm_directory + '/DTM_10m_unique_id.tif',\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=ids.shape[0],\n",
    "    width=ids.shape[1],\n",
    "    count=1,\n",
    "    dtype=np.dtype(np.int32),\n",
    "    crs=dtm_2021.crs,\n",
    "    transform=scaled_transform,\n",
    ") as dst:\n",
    "    dst.write(ids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use rioxarray again to plot the newly created 10m raster\n",
    "raster_10m = rioxarray.open_rasterio(\"DTM_10m_unique_id.tif\")\n",
    "print(raster_10m)\n",
    "raster_10m.plot()\n",
    "plt.xlabel(\"Easting (m)\")\n",
    "plt.ylabel(\"Northing (m)\")\n",
    "plt.title(\"10m raster with unique pixel ids\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f439fd",
   "metadata": {},
   "source": [
    "### 3.  Group all discrete returns based on the 10m pixel they fall into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, we assign unique ids created in the previous cell to each discrete return\n",
    "## Sample the unique ids from the 10m raster for each discrete return for the years 2019 and 2021\n",
    "## This sampling step might again take a few minutes\n",
    "\n",
    "raster_10m = rasterio.open(\"DTM_10m_unique_id.tif\")\n",
    "## For the year 2021\n",
    "dtm_id_vals_2021 = pd.DataFrame(list(rasterio.sample.sample_gen(raster_10m, coords_2021_sub)))\n",
    "\n",
    "## For the year 2019\n",
    "dtm_id_vals_2019 = pd.DataFrame(list(rasterio.sample.sample_gen(raster_10m, coords_2019_sub)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the unique ids extracted above with the discrete return dataframe (df_20xx) created earlier\n",
    "\n",
    "## Update 2019 df\n",
    "df_2019_with_ids = pd.concat([df_2019, dtm_id_vals_2019], axis=1)\n",
    "df_2019_with_ids.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \n",
    "                   \"ground_elevation\", \"discrete_ret_ht_above_ground\", \"uniq_id\"]\n",
    "df_2019_with_ids = df_2019_with_ids[df_2019_with_ids[\"ground_elevation\"] > -9999.0].reset_index(drop=True)\n",
    "\n",
    "## Update 2021 df\n",
    "df_2021_with_ids = pd.concat([df_2021, dtm_id_vals_2021], axis=1)\n",
    "df_2021_with_ids.columns = [\"x\", \"y\", \"z\", \"intensity\", \"return_num\", \"number_of_returns\", \"classification\", \n",
    "                            \"ground_elevation\", \"discrete_ret_ht_above_ground\", \"uniq_id\"]\n",
    "df_2021_with_ids = df_2021_with_ids[df_2021_with_ids[\"ground_elevation\"] > -9999.0].reset_index(drop=True)\n",
    "df_2021_with_ids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443e8be",
   "metadata": {},
   "source": [
    "The unique ids in the last column are not ordered as the discrete returns were sampled randomly. Before we calculate the relative height percentiles, let's look at the distribution of the number of discrete returns per 10m pixel for the years 2019 and 2021. A sufficient number of returns per 10m pixel will ensure that the height percentiles are robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8826d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "return_counts_per_10m_pixel_2019 = np.unique(df_2019_with_ids[\"uniq_id\"].sort_values(), return_counts=True)[1]\n",
    "return_counts_per_10m_pixel_2021 = np.unique(df_2021_with_ids[\"uniq_id\"].sort_values(), return_counts=True)[1]\n",
    "plt.hist(return_counts_per_10m_pixel_2019, bins=np.arange(0,400,10), density=True, alpha=0.5, color=\"blue\")\n",
    "plt.hist(return_counts_per_10m_pixel_2021, bins=np.arange(0,400,10), density=True, alpha=0.5, color=\"red\")\n",
    "plt.xlabel(\"# discrete returns per 10m pixel\", labelpad=10)\n",
    "plt.ylabel(\"Density\", labelpad=10)\n",
    "plt.title(\"Distribution of # of discrete returns per 10m pixel\", fontsize=12, pad=10)\n",
    "\n",
    "legend_elements = [Patch(facecolor=\"blue\", edgecolor=\"blue\", label='2019', alpha=0.5),\n",
    "                  Patch(facecolor=\"red\", edgecolor=\"red\", label='2021', alpha=0.5)]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cdb471",
   "metadata": {},
   "source": [
    "### 4. Calculate height percentiles of discrete returns ($20^{th}$, $30^{th}$, $50^{th}$, $75^{th}$, and $90^{th}$) for each 10m pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca39f08-e94e-4964-8495-cc1b1f419699",
   "metadata": {},
   "source": [
    "Before we calculate the height percentiles, we will drop 10-m pixels with too few discrete returns (< 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset the dataframes to include only the relevant columns\n",
    "df_2019_sub = pd.concat([df_2019_with_ids[\"x\"], df_2019_with_ids[\"y\"], df_2019_with_ids[\"uniq_id\"], df_2019[\"discrete_ret_ht_above_ground\"]], axis=1)\n",
    "df_2021_sub = pd.concat([df_2021_with_ids[\"x\"], df_2021_with_ids[\"y\"], df_2021_with_ids[\"uniq_id\"], df_2021[\"discrete_ret_ht_above_ground\"]], axis=1)\n",
    "\n",
    "## Calculate the number of returns per 10 m pixel\n",
    "return_counts_per_10m_pixel_2019 = pd.DataFrame(np.unique(df_2019_sub[\"uniq_id\"].sort_values(), return_counts=True)).transpose()\n",
    "return_counts_per_10m_pixel_2021 = pd.DataFrame(np.unique(df_2021_sub[\"uniq_id\"].sort_values(), return_counts=True)).transpose()\n",
    "return_counts_per_10m_pixel_2019.columns = return_counts_per_10m_pixel_2021.columns = [\"uniq_id\", \"number_of_returns\"]\n",
    "\n",
    "## Select only those 10m pixels which have greater than 50 returns\n",
    "return_counts_per_10m_pixel_2019 = return_counts_per_10m_pixel_2019[return_counts_per_10m_pixel_2019[\"number_of_returns\"] > 50].reset_index(drop=True)\n",
    "return_counts_per_10m_pixel_2021 = return_counts_per_10m_pixel_2021[return_counts_per_10m_pixel_2021[\"number_of_returns\"] > 50].reset_index(drop=True)\n",
    "valid_10m_pixels_2019 = return_counts_per_10m_pixel_2019[\"uniq_id\"]\n",
    "valid_10m_pixels_2021 = return_counts_per_10m_pixel_2021[\"uniq_id\"]\n",
    "\n",
    "## Update df_sub to include only valid 10m pixels (returns > 50)\n",
    "df_2019_sub = df_2019_sub[df_2019_sub[\"uniq_id\"].isin(valid_10m_pixels_2019)].reset_index(drop=True)\n",
    "df_2021_sub = df_2021_sub[df_2021_sub[\"uniq_id\"].isin(valid_10m_pixels_2021)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each unique id (uniq_id), calculate relative height percentiles (20th, 30th, 50th, 75th and 90th)\n",
    "\n",
    "## First define a function to calculate the percentile and return a percentile dataframe\n",
    "def calc_percentile(df,percentile):\n",
    "    ptile_df = pd.DataFrame(df.groupby(by=[\"uniq_id\"])[\"discrete_ret_ht_above_ground\"].quantile(percentile).reset_index(drop=True))\n",
    "    ptile_df[\"RH_ptile\"] = percentile*100\n",
    "    ptile_df.columns = [\"height\", \"RH_ptile\"]\n",
    "    return ptile_df\n",
    "\n",
    "ptile_20_2019 = calc_percentile(df_2019_sub,0.2)\n",
    "ptile_30_2019 = calc_percentile(df_2019_sub,0.3)\n",
    "ptile_50_2019 = calc_percentile(df_2019_sub,0.5)\n",
    "ptile_75_2019 = calc_percentile(df_2019_sub,0.75)\n",
    "ptile_90_2019 = calc_percentile(df_2019_sub,0.9)\n",
    "\n",
    "ptile_20_2021 = calc_percentile(df_2021_sub,0.2)\n",
    "ptile_30_2021 = calc_percentile(df_2021_sub,0.3)\n",
    "ptile_50_2021 = calc_percentile(df_2021_sub,0.5)\n",
    "ptile_75_2021 = calc_percentile(df_2021_sub,0.75)\n",
    "ptile_90_2021 = calc_percentile(df_2021_sub,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53044b0d",
   "metadata": {},
   "source": [
    "Combine these into single dataframes so we can plot all the data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0eb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_percentiles_2019 = pd.concat([ptile_20_2019, ptile_30_2019, ptile_50_2019, ptile_75_2019, ptile_90_2019], axis=0, ignore_index=True)\n",
    "rh_percentiles_2019[\"year\"] = 2019\n",
    "\n",
    "rh_percentiles_2021 = pd.concat([ptile_20_2021, ptile_30_2021, ptile_50_2021, ptile_75_2021, ptile_90_2021], axis=0, ignore_index=True)\n",
    "rh_percentiles_2021[\"year\"] = 2021\n",
    "\n",
    "## Combine 2019 and 2021 dataframes\n",
    "rh_percentiles_combined = pd.concat([rh_percentiles_2019, rh_percentiles_2021], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057e64b",
   "metadata": {},
   "source": [
    "### 5. Compare the distribution of height percentiles for the years 2019 and 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78437f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare boxplots\n",
    "plt.subplots(figsize=(8,5))\n",
    "sns.boxplot(x=rh_percentiles_combined[\"RH_ptile\"], y=rh_percentiles_combined[\"height\"], hue=rh_percentiles_combined[\"year\"],\n",
    "           fliersize=0, whis=[5,95])\n",
    "plt.ylim(0,15)\n",
    "plt.xlabel(\"Height percentile\", fontsize=12)\n",
    "plt.ylabel(\"Heights (in m)\", fontsize=12)\n",
    "plt.title(\"Relative Height Percentiles before (2019) and after (2021) the 2020 Creek Fire\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
